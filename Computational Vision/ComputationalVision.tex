\documentclass{article}
\usepackage{imakeidx}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{mathtools}
\graphicspath{{images/}}
\usepackage{geometry}
\geometry{a4paper,
total={170mm, 257mm},
left = 30mm,
right = 30mm,
bottom = 30mm,
top = 30mm
}



\usepackage{multicol}
\title{Computational Vision \linebreak Revision Notes}
\author{James Brown}
\makeindex
\begin{document}
	\pagenumbering{gobble}
	\maketitle
	\newpage
	\tableofcontents
	\newpage
	\pagenumbering{arabic}

	\section{Introduction}
	These are notes I have written in preparation of the 2017 Computation Vision exam. This year the module was run by Hamid Deghani (H.Dehghani@cs.bham.ac.uk).
	\linebreak \linebreak
	Computational vision is the acquisition of knowledge about objects and events in the environment through information processing of light emitted or reflected from objects. In short - we want to make a computer know what is where, by looking through information. We can also use computational vision to do automatic inference of properties of the world from images.

	\section{Human Vision}
	\par
	As humans we have evolved eyes which perceive the visible section of the electromagnetic spectrum, which falls between the wavelengths of 380nm - 760nm. Red light lies at the longer end (760nm) of visible light, and purple at the shorter end (380nm). \index{visible light}Visible light is strongly absorbed in the human eye because it can cause an electron to jump to a higher energy levels - yet it does not have enough energy to ionize cells. The evolutionary process of evolving eyes began more than 3 billion years ago with the formation of photopigments. These are molecules where light incident upon them will trigger a physical or chemical change. \index{photopigment}Photopigments capture photons which lead to the release of energy in the photopigment. This is may be used for photosynthesis or a behavioural reaction (a nerve reaction). A single \index{photocell}photocell contains multiple layers to catch light, not just one. This increases the chance of catching any one individual photon - if it's not caught by the first layer it's much more likely to be caught by the second and so on.

	\subsection{Image Formation}
	Photocells contain a light sensitive patch of photopigments. Using a single cell we can capture light in 1 dimension, but we can't really 'see'. All we can do is tell if the light is on, or off. With multiple cells we can have better direction resolution and with multiple cells we have a very wide aperture - we can't tell exactly where the image is. The image formed will be very bright but extremely fuzzy. This became curved over time and this which really helped with direction resolution as light incident on the left side of the curve must have come from the right. Images formed this way are still very blurry and will result in multiple projections of the same image. Over time eyes evolved to become \index{pinhole camera}pinhole cameras which form sharp yet dim images by allowing light to come from a single source (the pinhole) - effectively throwing away loads of potential information about the image. The solution to form sharp and bright images was to use a \index{lens}lens at the front of the eye. The lens focuses all incoming light to a single point and from there we can use our simple pinhole camera for forming images. It should be noted that the image formed is upside down to what exists in reality.
	
	\subsection{Retina Processing}
	The human \index{retina}retina contains two kinds of \index{photoreceptors}photoreceptors which respond to incident light - \index{rods}\textbf{rods}  (around 120 million) and \index{cones}\textbf{cones} (around 6 million). Rods are extremely sensitive photosensors and respond to a single photon of light. Multiple rod cells converge to the same \index{ganglion cell}ganglion cell and therefore neuron within the \index{retina} which results in poor spatial resolution. Rods are responsible simply for detecting the presence of light, and as a result make up the entirety of our night-vision. On the other hand, cones are active at much higher light levels and responsible for the detection of different colours of light. Cones have a much higher resolution as they are processed by several different neurons. Within the eye we have a \index{receptive field}receptive field, which is the area on which light must fall for a neuron to be stimulated. It should be noted that receptive field in the center of the eye is much smaller than it is for the periphery of the eye.
	
	\subsection{The Visual Pathway}
	Vision is generated by photoreceptors in the retina. All the information captured leaves the eye by way of the optic nerve\index{optic nerve}. There is a partial crossing of axons at the optic chiasm\index{chiasm}. This is only partial as information from both eyes is sent to both sides of the brain - this allows us to process depth. After this chiasm, the axons are called the optic tract. The optic tract wraps around the midbrain to get to the lateral geniculate nucleus (LGN)\index{lateral geniculate nucleus}. The LGN axons fan out through the deep white matter of the brain and ultimately to the visual cortex\index{visual cortex}.
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.6\textwidth]{visual_pathway}
		\caption{The visual pathway in the human brain}
		\label{fig:visual pathway}
	\end{figure}
	
	\subsection{Colour Processing}
	Objects in the world selectively absorb some wavelengths (a colour) and reflect other wavelengths. Human retinas contain three different types of cones to respond to different colours. This gives us the ability to distinguish different forms of the same object - for example: unripe, ripe and off fruit. Many theories of colour vision have been proposed and some have been hard to disprove until recently. 
	\par	
	In 1802 Young proposed that the eye has three different types of receptors, each of which are sensitive to a single hue. From this he proposed that any colour can be produced by appropriate mixing of the three primary colours. This is known as the trichromatic theory\index{trichromatic theory}.
	
	\par 
	Hering suggested that colour may be represented in a visual system as 'opponent colours' in the 1930's.
	
	
	\section{Edge Detection}
	\subsection{Intensity Images}
	An intensity image\index{intensity image} is a data matrix whose values represent intensities within some range. Each element of the matrix corresponds to one image pixel\index{pixel}. An indexed image consists of a data matrix, $X$, and a colour map\index{colour map} matrix, $map$. $map$ is a $m$-by-3 array of double containing floating point values in the range $[0, 1]$. Each row in the map specifies the red, green an blue components of a single color. Each cell in the indexed image then specifies the corresponding colour from the colour map. In an intensity image can be thought of as a function $f(x, y)$ mapping coordinates to intensity. From this we can calculate an intensity gradient\index{intensity gradient}.
	\begingroup
	\renewcommand*{\arraystretch}{1.5}
	\[ \overrightarrow{G}[f(x,y)] = \begin{bmatrix}G_{x} \\ G_{y} \end{bmatrix} = \begin{bmatrix} \frac{df}{dx} \\ \frac{df}{dy} \end{bmatrix} \]	
	\endgroup

	 This is a vector which we can think of as having an $x$ and $y$ component. We can calculate both the magnitude and direction for this gradient of intensity. 
	\begin{multicols}{2}
		\noindent
		\centering
			$M(\overrightarrow{G}) = \sqrt{G_{x}^{2} + G_{y}^{2}}$ \\
			$\alpha(x, y) = \tan^{-1}\left(\frac{G_y}{G_x}\right)$
	\end{multicols}
	
	\subsection{Approximating the Intensity Gradient}
	
	\subsection{Filtering}
	
	\section{Noise Filtering}
	When taking images we also gather a lot of noise which results in fake edges once we apply our edge detectors. We would like to remove this noise and we many filters which can be implemented by the idea of convolution. The most widely used of these filters is the Gaussian filter\index{Gaussian filter}.

	\section{Advanced Edge Detection}

	\section{Hough Transform}

	\section{Scale Invariant Feature Transform}

	\section{Face Recognition}

	\section{Motion}

	\section{ROC Analysis}
	Receiver operating characteristic analysis provides us tools to select possibly optimal models and to discard suboptimal ones. In order to carry out ROC analysis we need to count the different kind of errors that are possible. When classifying if a pixel is an edge or not, there are 4 possible situations - \textbf{true positive} (predicted it was an edge and it was), \textbf{false positive} (predicted it was an edge and it wasn't), \textbf{false negative} (predicted it wasn't an edge and it actually was) and \textbf{true negative} (predicted it wasn't an edge and it wasn't).
	
Once classifying each pixel, we then count the total number of each label (TP, FP, TN, FN) over the large dataset. In ROC analysis we use two statistics:
	\begin{multicols}{2}
		\noindent
		\centering
			$ \text{Sensitivity} = \frac{TP}{TP+FN}$ \\
			$ \text{Specificity} = \frac{TN}{TN+FP}$
	\end{multicols}	
	Sensitivity is the likelihood of spotting a positive case when presented with one or the proportion of edges we find. Specificity is the likelihood of spotting a negative case when presented with one or the proportion of non edges that we find. To define a ROC space we only need the true positive rate (TPR) and the false positive rate (FPR). The TPR is simply the sensitivity and the FPR is $1 - \text{specificity}$. We use the FPR and TPR as x and y axes respectively of the ROC space.
	
	\begin{wrapfigure}{r}{0.4\textwidth}
		\centering
		\includegraphics[width=0.4\textwidth]{roc_analysis}
		\caption{A sample ROC space}
		\label{fig:roc_analysis}
	\end{wrapfigure}
	
	All of the optimal detectors lie on the convex hull - the best possible position is for a detector to lie in the top left corner. It should be noted that if the edge detector lies below the dotted line then we can instantly make the detector better by simply just flipping the output of the algorithm!
		
	\section{Object Recognition}

	\section{Model Based Object Recognition}
	\newpage
	\listoffigures
	\printindex

\end{document}
